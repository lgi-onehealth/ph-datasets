{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ph-datasets: A Nextflow workflow for downloading benchmarking public health datasets Background As part of validating bioinformatic pipelines, we need benchmarking and ground truth datasets for our pipelines. Timme et al . (2017) published a first set of public health datasets for outbreak investigation using genomic data in foodborne bacterial pathogens. As part of the publication, they created a GitHub repository with descritptions of the datasets they described in their papers (https://github.com/WGS-standards-and-analysis/datasets). A number of organisations have since forked this repository, and added their own data. Here, we provide a Nextflow workflow that will download the datasets described in the paper, and hosted in the original GitHub repository. We have also added a few datasets from some of the forked repositories. Source repositories: https://github.com/WGS-standards-and-analysis/datasets https://github.com/globalmicrobialidentifier-WG3/datasets https://github.com/CDCgov/datasets-sars-cov-2 References Timme, R. E., Rand, H., Shumway, M., Trees, E. K., Simmons, M., Agarwala, R., Davis, S., Tillman, G. E., Defibaugh-Chavez, S., Carleton, H. A., Klimke, W. A., & Katz, L. S. (2017). Benchmark datasets for phylogenomic pipeline validation, applications for foodborne pathogen surveillance. PeerJ, 5, e3893. https://peerj.com/articles/3893/","title":"Home"},{"location":"#ph-datasets-a-nextflow-workflow-for-downloading-benchmarking-public-health-datasets","text":"","title":"ph-datasets: A Nextflow workflow for downloading benchmarking public health datasets"},{"location":"#background","text":"As part of validating bioinformatic pipelines, we need benchmarking and ground truth datasets for our pipelines. Timme et al . (2017) published a first set of public health datasets for outbreak investigation using genomic data in foodborne bacterial pathogens. As part of the publication, they created a GitHub repository with descritptions of the datasets they described in their papers (https://github.com/WGS-standards-and-analysis/datasets). A number of organisations have since forked this repository, and added their own data. Here, we provide a Nextflow workflow that will download the datasets described in the paper, and hosted in the original GitHub repository. We have also added a few datasets from some of the forked repositories. Source repositories: https://github.com/WGS-standards-and-analysis/datasets https://github.com/globalmicrobialidentifier-WG3/datasets https://github.com/CDCgov/datasets-sars-cov-2","title":"Background"},{"location":"#references","text":"Timme, R. E., Rand, H., Shumway, M., Trees, E. K., Simmons, M., Agarwala, R., Davis, S., Tillman, G. E., Defibaugh-Chavez, S., Carleton, H. A., Klimke, W. A., & Katz, L. S. (2017). Benchmark datasets for phylogenomic pipeline validation, applications for foodborne pathogen surveillance. PeerJ, 5, e3893. https://peerj.com/articles/3893/","title":"References"},{"location":"installation/","text":"Installing Nextflow The pipeline is written in Nextflow, and requires it to be able to run. Native install Good instructions on installing Nextflow can be found here: https://www.nextflow.io/docs/latest/getstarted.html . Using conda If you are using Conda, you can install Nextflow into an existing environment with the following command: conda install -c conda-forge -c bioconda nextflow Or, you can create a new environment with the following: conda create -c conda-forge -c bioconda -n nextflow nextflow Running it with Docker If you do not wish to install Nextflow, you can run it with Docker. Here is an example command to list all the available datasets using Nextflow installed on Docker: docker run -it -v $( pwd ) :/data nextflow run lgi-onehealth/ph-datasets -profile docker --list Running a test Once you have nextflow installed, you can run a small test to make sure it is working. mkdir test_run && cd test_run nextflow run lgi-onehealth/ph-datasets -profile test [ ,<conda | docker> ] To run the test, you need to set -profile to test . Note If you don't have the tools installed in your $PATH , you can also set -profile to test,docker to run with Docker or test,conda to run with Conda. This will download a small dummy dataset created to test the pipeline. It consists of sequence data for two Zika virus samples. The files are about 2MB in size each. The test run takes about 15s to run on my laptop with a standard WiFi connection. If you are running with Conda for the first time, it may take a little longer as it will need to create the appropriate conda environments.","title":"Installation"},{"location":"installation/#installing-nextflow","text":"The pipeline is written in Nextflow, and requires it to be able to run.","title":"Installing Nextflow"},{"location":"installation/#native-install","text":"Good instructions on installing Nextflow can be found here: https://www.nextflow.io/docs/latest/getstarted.html .","title":"Native install"},{"location":"installation/#using-conda","text":"If you are using Conda, you can install Nextflow into an existing environment with the following command: conda install -c conda-forge -c bioconda nextflow Or, you can create a new environment with the following: conda create -c conda-forge -c bioconda -n nextflow nextflow","title":"Using conda"},{"location":"installation/#running-it-with-docker","text":"If you do not wish to install Nextflow, you can run it with Docker. Here is an example command to list all the available datasets using Nextflow installed on Docker: docker run -it -v $( pwd ) :/data nextflow run lgi-onehealth/ph-datasets -profile docker --list","title":"Running it with Docker"},{"location":"installation/#running-a-test","text":"Once you have nextflow installed, you can run a small test to make sure it is working. mkdir test_run && cd test_run nextflow run lgi-onehealth/ph-datasets -profile test [ ,<conda | docker> ] To run the test, you need to set -profile to test . Note If you don't have the tools installed in your $PATH , you can also set -profile to test,docker to run with Docker or test,conda to run with Conda. This will download a small dummy dataset created to test the pipeline. It consists of sequence data for two Zika virus samples. The files are about 2MB in size each. The test run takes about 15s to run on my laptop with a standard WiFi connection. If you are running with Conda for the first time, it may take a little longer as it will need to create the appropriate conda environments.","title":"Running a test"},{"location":"usage/","text":"TLDR List the datasets, and specify the dataset to download using the --key flag nextflow run lgi-onehealth/ph-datasets --list nextflow run lgi-onehealth/ph-datasets [ -profile <conda | docker> ] --key Se-1203NYJAP-1 The basic workflow A typical run of the workflow will have two steps. First, you list the available datasets: nextflow run lgi-onehealth/ph-datasets --list Here is an excerpt of what the output will look like: --------------------------------------------- key: Se-1203NYJAP-1 organism: Salmonella enterica description: Salmonella enterica serovar Bareilly 1203NYJAP-1 Tuna Scrape Outbreak --------------------------------------------- key: Lm-1408MLGX6-3WGS organism: Listeria monocytogenes description: Listeria monocytogenes stone fruit outbreak --------------------------------------------- ... --------------------------------------------- key: SC2-nonvoivoc organism: SARS-CoV-2 description: SARS-CoV-2 non-VOI/VOC sequence data --------------------------------------------- key: SC2-voivoc organism: SARS-CoV-2 description: SARS-CoV-2 VOI/VOC sequence data --------------------------------------------- In the second step, one uses the key of the dataset they wish to download to tell the workflow which data to retrieve. Assuming you decided on the SC2-voivoc dataset, this is done with the following command: nextflow run lgi-onehealth/ph-datasets --key SC2-voivoc [ -profile <conda | docker> ] Note The -profile portion is optional. You should use it if you don't have all the tools installed in your $PATH . Update the workflow To ensure you are always up to date with the latest version of the workflow, you can add the -latest flag to the command. For example: nextflow run lgi-onehealth/ph-datasets -latest [ -profile <conda | docker> ] --key SC2-voivoc Run a specific version of the workflow To specify a specific commit or tag of the workflow to run, use the -r option. For example: nextflow run lgi-onehealth/ph-datasets -r v0.1.0 [ -profile <conda | docker> ] --key SC2-voivoc The workflow outputs The workflow will create, by default, a folder called datasets in the directory that you run the workflow from. In the that folder, there will be three subfolders: fastq - contains the fastq files organised into subfolders named after the accession, as well as the JSON output from phcue-ck with the URL information for each FASTQ file metadata - contains three files: metadata.csv - contains the metadata for the dataset as provided by the repository, with one row per sample/accession metadataset.yml - contains the key:value metadata for the dataset as provided in the header of the datafile in the repository samplesheet.csv - contains the full path for each downloaded FASTQ file with one row per accession (this is useful for downstream analyses) pipeline_info - contains the pipeline runtime report, timeline, trace, and DAG, as well as a yaml with all the software versions used while executing the pipeline. The folder structure for the test example looks like this: datasets \u251c\u2500\u2500 fastq \u2502 \u251c\u2500\u2500 SRR17231514 \u2502 \u2502 \u251c\u2500\u2500 SRR17231514_1.fastq.gz \u2502 \u2502 \u251c\u2500\u2500 SRR17231514_2.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 SRR17231514_phcueck.json \u2502 \u2514\u2500\u2500 SRR17231530 \u2502 \u251c\u2500\u2500 SRR17231530_1.fastq.gz \u2502 \u251c\u2500\u2500 SRR17231530_2.fastq.gz \u2502 \u2514\u2500\u2500 SRR17231530_phcueck.json \u251c\u2500\u2500 metadata \u2502 \u251c\u2500\u2500 metadata.csv \u2502 \u251c\u2500\u2500 metadataset.yml \u2502 \u2514\u2500\u2500 samplesheet.csv \u2514\u2500\u2500 pipeline_info \u251c\u2500\u2500 execution_report_2022-08-25_09-26-55.html \u251c\u2500\u2500 execution_timeline_2022-08-25_09-26-55.html \u251c\u2500\u2500 execution_trace_2022-08-25_09-26-55.txt \u251c\u2500\u2500 execution_trace_2022-08-25_09-40-02.txt \u251c\u2500\u2500 pipeline_dag_2022-08-25_09-26-55.html \u2514\u2500\u2500 software_versions.yml Workflow parameters Parameter Value Description --key <string> The key of the dataset to download --list <boolean> List the available datasets --outdir <string> The output directory (default: datasets ) --tracedir <string> Where to store pipeline trace reports (default: $OUTDIR/pipeline_info ) --max-downloads <int> Maximum number of simultaenous downloads to run (default: 6) --publish-mode <striing> Mode to publish data into the $OUTDIR (default: copy). Options are: copy, symlink, move, link, relink, copyNoFollow 1 The different options are described in the Nextflow documentation here \u2013 scroll down to the Table of publish modes . \u21a9","title":"Usage"},{"location":"usage/#tldr","text":"List the datasets, and specify the dataset to download using the --key flag nextflow run lgi-onehealth/ph-datasets --list nextflow run lgi-onehealth/ph-datasets [ -profile <conda | docker> ] --key Se-1203NYJAP-1","title":"TLDR"},{"location":"usage/#the-basic-workflow","text":"A typical run of the workflow will have two steps. First, you list the available datasets: nextflow run lgi-onehealth/ph-datasets --list Here is an excerpt of what the output will look like: --------------------------------------------- key: Se-1203NYJAP-1 organism: Salmonella enterica description: Salmonella enterica serovar Bareilly 1203NYJAP-1 Tuna Scrape Outbreak --------------------------------------------- key: Lm-1408MLGX6-3WGS organism: Listeria monocytogenes description: Listeria monocytogenes stone fruit outbreak --------------------------------------------- ... --------------------------------------------- key: SC2-nonvoivoc organism: SARS-CoV-2 description: SARS-CoV-2 non-VOI/VOC sequence data --------------------------------------------- key: SC2-voivoc organism: SARS-CoV-2 description: SARS-CoV-2 VOI/VOC sequence data --------------------------------------------- In the second step, one uses the key of the dataset they wish to download to tell the workflow which data to retrieve. Assuming you decided on the SC2-voivoc dataset, this is done with the following command: nextflow run lgi-onehealth/ph-datasets --key SC2-voivoc [ -profile <conda | docker> ] Note The -profile portion is optional. You should use it if you don't have all the tools installed in your $PATH .","title":"The basic workflow"},{"location":"usage/#update-the-workflow","text":"To ensure you are always up to date with the latest version of the workflow, you can add the -latest flag to the command. For example: nextflow run lgi-onehealth/ph-datasets -latest [ -profile <conda | docker> ] --key SC2-voivoc","title":"Update the workflow"},{"location":"usage/#run-a-specific-version-of-the-workflow","text":"To specify a specific commit or tag of the workflow to run, use the -r option. For example: nextflow run lgi-onehealth/ph-datasets -r v0.1.0 [ -profile <conda | docker> ] --key SC2-voivoc","title":"Run a specific version of the workflow"},{"location":"usage/#the-workflow-outputs","text":"The workflow will create, by default, a folder called datasets in the directory that you run the workflow from. In the that folder, there will be three subfolders: fastq - contains the fastq files organised into subfolders named after the accession, as well as the JSON output from phcue-ck with the URL information for each FASTQ file metadata - contains three files: metadata.csv - contains the metadata for the dataset as provided by the repository, with one row per sample/accession metadataset.yml - contains the key:value metadata for the dataset as provided in the header of the datafile in the repository samplesheet.csv - contains the full path for each downloaded FASTQ file with one row per accession (this is useful for downstream analyses) pipeline_info - contains the pipeline runtime report, timeline, trace, and DAG, as well as a yaml with all the software versions used while executing the pipeline. The folder structure for the test example looks like this: datasets \u251c\u2500\u2500 fastq \u2502 \u251c\u2500\u2500 SRR17231514 \u2502 \u2502 \u251c\u2500\u2500 SRR17231514_1.fastq.gz \u2502 \u2502 \u251c\u2500\u2500 SRR17231514_2.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 SRR17231514_phcueck.json \u2502 \u2514\u2500\u2500 SRR17231530 \u2502 \u251c\u2500\u2500 SRR17231530_1.fastq.gz \u2502 \u251c\u2500\u2500 SRR17231530_2.fastq.gz \u2502 \u2514\u2500\u2500 SRR17231530_phcueck.json \u251c\u2500\u2500 metadata \u2502 \u251c\u2500\u2500 metadata.csv \u2502 \u251c\u2500\u2500 metadataset.yml \u2502 \u2514\u2500\u2500 samplesheet.csv \u2514\u2500\u2500 pipeline_info \u251c\u2500\u2500 execution_report_2022-08-25_09-26-55.html \u251c\u2500\u2500 execution_timeline_2022-08-25_09-26-55.html \u251c\u2500\u2500 execution_trace_2022-08-25_09-26-55.txt \u251c\u2500\u2500 execution_trace_2022-08-25_09-40-02.txt \u251c\u2500\u2500 pipeline_dag_2022-08-25_09-26-55.html \u2514\u2500\u2500 software_versions.yml","title":"The workflow outputs"},{"location":"usage/#workflow-parameters","text":"Parameter Value Description --key <string> The key of the dataset to download --list <boolean> List the available datasets --outdir <string> The output directory (default: datasets ) --tracedir <string> Where to store pipeline trace reports (default: $OUTDIR/pipeline_info ) --max-downloads <int> Maximum number of simultaenous downloads to run (default: 6) --publish-mode <striing> Mode to publish data into the $OUTDIR (default: copy). Options are: copy, symlink, move, link, relink, copyNoFollow 1 The different options are described in the Nextflow documentation here \u2013 scroll down to the Table of publish modes . \u21a9","title":"Workflow parameters"}]}